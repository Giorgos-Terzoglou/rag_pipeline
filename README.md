# ğŸ“Œ RAG System for IMF Reports

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline designed to extract **key numbers, statistics, and trends** from IMF reports.  
It integrates:
- **Web Crawling** of IMF content (HTML, PDFs, tables)
- **Data Cleaning** (remove boilerplate, non-English text)
- **Hybrid Search** (BM25 + Vector Search) with Cross-Encoder reranking
- **Local LLM (Llama 3.1 8B via Ollama)** for high-quality answers.

---

## ğŸš€ Pipeline Overview

The RAG system follows these main stages:

---

### **1ï¸âƒ£ Data Collection (Web Crawler + PDF Loader + Table Extractor)**

We automatically collect IMF content from the official site:

- **`web_crawler.py`** crawls IMF pages and detects:
  - HTML pages (text)
  - PDF reports (linked in the site)
- **`pdf_loader.py`** downloads and extracts:
  - Full PDF text
  - Tables (CSV format) from PDFs
- **`table_extractor.py`** extracts tables embedded in HTML

Run ingestion:
```bash
python ingest_pipeline.py --start-url "https://www.imf.org/en/Publications" --max-pages 50
```
Outputs:
- `data/output/collected_data.json` â€” combined text + tables
- `data/output/data_log.csv` â€” provenance log
- `data/output/failed_links.csv` â€” failed fetch attempts
- `data/pdf_texts/` â€” extracted PDF text files
- `data/pdf_tables/` â€” extracted PDF tables (CSV)

---

### **2ï¸âƒ£ Data Cleaning (Boilerplate & Language Filtering)**

After ingestion, remove **boilerplate phrases** and **non-English entries**:

- **`clean_json_for_rag.py`**:
  - Detects language (`langdetect`)
  - Removes common IMF boilerplate patterns
  - Outputs a cleaned dataset for embeddings

Run cleaning:
```bash
python clean_json_for_rag.py
```

Outputs:
- `data/output/collected_data_clean.json` â€” cleaned, English-only dataset

---

### **3ï¸âƒ£ Targeted Semantic Chunking**
Chunks cleaned text into **semantic segments** to preserve numbers, statistics, and context.
```bash
python run_graph.py --build
```
Outputs:
- Targeted chunks ready for embedding and storage in Qdrant

---

### **4ï¸âƒ£ Hybrid Search (BM25 + Vector Search)**
Retrieves documents using:
- **BM25** (keyword search from Qdrant payload)
- **Vector Search** (semantic similarity)
- **Cross-Encoder Reranker** (precision improvement)
```bash
python run_graph.py --query "Extract IMF's latest numbers and trends"
```

---

### **5ï¸âƒ£ Query the LLM (Llama 3.1 8B via Ollama)**
Pass ranked context to local LLM:
```bash
ollama pull llama3.1:8b
python run_graph.py --query "Extract IMF's latest numbers and trends"
```

Example Output:
```
Numbers:
- 191 member countries
- $1 trillion lending capacity
- 9 (11%) actions completed
- 34 (42%) actions underway

Trends:
- Moderate progress on IMF initiatives
- Focus on financial stability and trade growth
```

---

## ğŸ“¡ API Access (FastAPI)
Expose the RAG system as an API:
```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000
```
Swagger Docs:
ğŸ‘‰ [http://localhost:8000/docs](http://localhost:8000/docs)

---

## ğŸ— Project Structure
```
project/
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ pdf_tables/
â”‚   â”œâ”€â”€ pdf_texts/
â”‚   â””â”€â”€ pdfs/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ web_crawler.py
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”‚   â”œâ”€â”€ table_extractor.py
â”‚   â”‚   â””â”€â”€ ingest_pipeline.py
â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â”œâ”€â”€ build_graph.py
â”‚   â”‚   â””â”€â”€ nodes.py
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ bm25_search.py
â”‚   â”‚   â”œâ”€â”€ hybrid_search.py
â”‚   â”‚   â””â”€â”€ reranker.py
â”‚   â””â”€â”€ llm/
â”‚       â””â”€â”€ rag_pipeline.py
â”‚â”€â”€ api/
â”‚   â””â”€â”€ main.py
â”‚â”€â”€ clean_json_for_rag.py
â”‚â”€â”€ run_graph.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ README.md
```

---

## ğŸ”‘ Key Components
- **Web Scraper** (HTML, PDFs, Tables)
- **Data Cleaner** (Language filter + boilerplate removal)
- **Qdrant** â€” Vector DB for document storage + BM25
- **Hybrid Retriever** â€” Semantic + lexical search
- **Cross-Encoder Reranker** â€” Improves ranking
- **Ollama + Llama 3.1** â€” Local LLM inference
