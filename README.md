# ğŸ“Œ RAG System for IMF Reports

This project implements a **Retrieval-Augmented Generation (RAG)** system that scrapes IMF data, chunks it, embeds it, stores it in **Qdrant**, and retrieves it using **Hybrid Search (BM25 + Vector Search)** with reranking before passing context to an **LLM (Llama 3.1 8B via Ollama)**.

---

## ğŸš€ Pipeline Overview
The pipeline consists of the following steps:

### 1ï¸âƒ£ Data Collection & Cleaning
We ingest IMF-related content (documents, reports, factsheets) and clean it.
```bash
python ingest_pipeline.py --source ./data/input/imf_data.json
```
Outputs:
- `data/output/cleaned_data.json` â€” cleaned and normalized JSON data

### 2ï¸âƒ£ Targeted Semantic Chunking
We chunk IMF documents using **targeted semantic segmentation** to capture meaningful sentences (e.g., numbers, trends, tables) instead of fixed-size tokens.
```bash
python run_graph.py --build
```
Outputs:
- `1459 targeted chunks` created from documents

### 3ï¸âƒ£ Embedding Generation
We generate embeddings for each chunk using:
- **SentenceTransformers (all-MiniLM-L6-v2)** *(local, free)*
- (Optional) **OpenAI text-embedding-3-small** *(cloud, paid)*
```python
vectors = embed_documents(chunks, use_openai=False)
```
Outputs:
- `1459 vectors of dimension 384`

### 4ï¸âƒ£ Local Similarity Search Test (FAISS)
Before uploading to Qdrant, test embeddings locally:
```python
results = similarity_search(query="What is the IMF World Economic Outlook?", k=3)
```
This ensures embeddings are relevant.

### 5ï¸âƒ£ Upload to Qdrant
Upload vectors and metadata to **Qdrant**:
```python
upload_to_qdrant(vectors, docs, collection_name="rag_collection")
```
Run Qdrant locally:
```bash
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
```

### 6ï¸âƒ£ Retrieval (Hybrid Search: Vector + BM25 from Qdrant)
We retrieve relevant documents using **Hybrid Retrieval**:
- **BM25**: Keyword-based scoring (from Qdrant text payload)
- **Vector Search**: Semantic similarity via embeddings
- **Reranking**: Cross-encoder reorders top candidates for better accuracy
```python
candidates = hybrid_retriever.search(question, k=15)
ranked = reranker.rerank(question, candidates, top_k=5)
```

### 7ï¸âƒ£ Query the LLM (Llama 3.1 8B via Ollama)
The ranked context is passed to the LLM:
```python
ollama pull llama3.1:8b
python run_graph.py --query "Extract IMF's latest numbers and trends"
```
The retrieved context is sent to the LLM along with the user question.

---

## ğŸ“¡ Example Usage
### Build Qdrant Index
```bash
python run_graph.py --build
```

### Query the RAG System
```bash
python run_graph.py --query "Extract IMF's latest numbers and trends"
```

Example output:
```
Numbers:
- 191 member countries
- $1 trillion lending capacity
- 9 (11%) actions completed
- 34 (42%) actions underway

Trends:
- Moderate progress on IMF initiatives
- Significant focus on financial stability and trade growth
```

---

## ğŸ“Œ Next Steps for Production-Ready RAG System
- âœ… Improve **chunking** to better capture numbers & trends
- âœ… Add **Hybrid Search** (BM25 from Qdrant + FAISS)
- ğŸ”„ Add **evaluation scripts** to measure retrieval accuracy
- ğŸ”„ Implement **prompt templates** for structured responses (tables, JSON)
- ğŸ”„ Create **FastAPI endpoint** for external queries

---

## ğŸ— Project Structure
```
project/
â”‚â”€â”€ data/
â”‚   â”‚â”€â”€ input/
â”‚   â”‚â”€â”€ output/
â”‚â”€â”€ src/
â”‚   â”‚â”€â”€ graph/
â”‚   â”‚   â”‚â”€â”€ build_graph.py
â”‚   â”‚   â”‚â”€â”€ nodes.py
â”‚   â”‚â”€â”€ retrieval/
â”‚   â”‚   â”‚â”€â”€ bm25_search.py
â”‚   â”‚   â”‚â”€â”€ hybrid_search.py
â”‚   â”‚   â”‚â”€â”€ reranker.py
â”‚   â”‚â”€â”€ llm/
â”‚   â”‚   â”‚â”€â”€ rag_pipeline.py
â”‚â”€â”€ run_graph.py
â”‚â”€â”€ README.md
```

---

## ğŸ”‘ Key Components
- **Qdrant** â€” Vector database for embeddings & metadata
- **BM25** â€” Keyword search from Qdrant payload
- **FAISS** â€” Local similarity search for testing
- **Hybrid Retriever** â€” Combines BM25 & vector search
- **Reranker** â€” Improves retrieval precision
- **Llama 3.1 8B** â€” Local LLM for answer generation via Ollama
