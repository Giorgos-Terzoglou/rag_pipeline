# ğŸ“Œ RAG System for IMF Reports

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline designed to extract **key numbers, statistics, and trends** from IMF reports. It integrates **Hybrid Search (BM25 + Vector Search)** with **Cross-Encoder reranking** and a **local LLM (Llama 3.1 8B via Ollama)** for high-quality answers.

---

## ğŸš€ Pipeline Overview

The RAG system follows these main stages:

### **1ï¸âƒ£ Data Collection & Cleaning**
- Ingest IMF-related content (reports, factsheets, statistics)
- Clean and normalize data into structured JSON
```bash
python ingest_pipeline.py --source ./data/input/imf_data.json
```
Outputs:
- `data/output/cleaned_data.json` â€” Cleaned & normalized IMF dataset

---

### **2ï¸âƒ£ Targeted Semantic Chunking**
- Chunk text into **semantic segments** using `SemanticChunker` to capture context-rich passages (numbers, tables, trends)
```bash
python run_graph.py --build
```
Outputs:
- `1459 targeted chunks` created from reports

---

### **3ï¸âƒ£ Embedding Generation**
- Embeddings generated for each chunk using:
  - **SentenceTransformers (all-MiniLM-L6-v2)** *(local, free)*
  - (Optional) **OpenAI text-embedding-3-small** *(cloud, paid)*
```python
vectors = embed_documents(chunks, use_openai=False)
```
Outputs:
- `1459 vectors (dim=384)` stored with metadata

---

### **4ï¸âƒ£ Vector Search Test (FAISS)**
- Before uploading to Qdrant, validate embeddings locally:
```python
results = similarity_search(query="What is the IMF World Economic Outlook?", k=3)
```
Ensures embeddings retrieve relevant chunks.

---

### **5ï¸âƒ£ Upload to Qdrant**
- Store vectors & metadata in Qdrant:
```bash
docker run -p 6333:6333 qdrant/qdrant
python upload_to_qdrant.py
```
Collection: `rag_collection`

---

### **6ï¸âƒ£ Hybrid Search (BM25 + Vector Search)**
- Retrieve documents using **Hybrid Retriever**:
  - **BM25**: Keyword-based search from Qdrant payload
  - **Vector Search**: Semantic similarity from embeddings
  - **Cross-Encoder Reranker**: Reorders top results for precision
```python
candidates = hybrid_retriever.search(question, k=15)
ranked = reranker.rerank(question, candidates, top_k=5)
```

---

### **7ï¸âƒ£ Query the LLM (Llama 3.1 8B via Ollama)**
- Ranked context passed to local LLM for final answer:
```bash
ollama pull llama3.1:8b
python run_graph.py --query "Extract IMF's latest numbers and trends"
```

Example Output:
```
Numbers:
- 191 member countries
- $1 trillion lending capacity
- 9 (11%) actions completed
- 34 (42%) actions underway

Trends:
- Moderate progress on IMF initiatives
- Focus on financial stability and trade growth
```

---

## ğŸ“¡ API Access (FastAPI)

The RAG system is also exposed via an API for production use:

Run API:
```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000
```

Swagger Docs:
ğŸ‘‰ [http://localhost:8000/docs](http://localhost:8000/docs)

---

## ğŸ— Project Structure
```
project/
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â””â”€â”€ output/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â”œâ”€â”€ build_graph.py
â”‚   â”‚   â””â”€â”€ nodes.py
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ bm25_search.py
â”‚   â”‚   â”œâ”€â”€ hybrid_search.py
â”‚   â”‚   â””â”€â”€ reranker.py
â”‚   â””â”€â”€ llm/
â”‚       â””â”€â”€ rag_pipeline.py
â”‚â”€â”€ api/
â”‚   â””â”€â”€ main.py
â”‚â”€â”€ run_graph.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ README.md
```

---

## ğŸ”‘ Key Components
- **Qdrant** â€” Vector DB for document storage and BM25 text payload
- **BM25** â€” Keyword search from Qdrant payload
- **FAISS** â€” Local similarity search for pre-upload testing
- **Hybrid Retriever** â€” Combines semantic + lexical retrieval
- **Cross-Encoder Reranker** â€” Reorders retrieved documents
- **Ollama + Llama 3.1** â€” Fast local inference using LLMs
